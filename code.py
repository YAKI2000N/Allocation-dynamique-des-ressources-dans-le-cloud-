# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgTt43yXIVBaM2wpCdtxWBC1D93ZS0rV
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson

#System parameters :
K = 5  # Maximum number of VMs
B = 15  # System capacity (queue + number of VMs)
d = 1  # Throughput of each VM
b = 5  # Maximum batch size of packet arrivals
lambda_rate = 2  # Average rate of packet arrivals per time slot

PA = [poisson.pmf(k, lambda_rate) for k in range(b + 1)]
PA /= np.sum(PA)

#PA = np.full(b + 1, 1 / (b + 1))  # Uniform distribution for simplicity

#Initializing Costs
Cf = 1  # Cost of running a VM per slot
Ca = 5  # Cost of activating a VM
Cd = 3  # Cost of deactivating a VM
CH = 2  # Cost of maintaining a client in the queue

#Représentation de la MDP :
state_space_size = (B + 1) * (K + 1)  # Encoding states as integers
action_space_size = K+1
Q = np.zeros((state_space_size, action_space_size)) # initialize Q table

#function allowing the update of the state index
def get_state_index(num_clients, num_active_vms):
    return num_clients * (K + 1) + num_active_vms

#Simulate the state transition and calculate reward
def take_action(state_index, action):
    m, n = divmod(state_index, K + 1)  # Decode the current state (m, n) from state index
    i = np.random.choice(range(len(PA)), p=PA) #Simulate packet arrivals based on PA

    # Calculate the next state based on the given action and packet arrivals
    next_m = min(B, max(0, m + i - action * d))
    next_n = action
    next_state_index = get_state_index(next_m, next_n)

    # Calculate the reward (cost to minimize)
    reward = action * Cf + (action - n) * (action > n) * Ca + (n - action) * (action < n) * Cd + m * CH

    return next_state_index, -reward  # Return negative reward because we aim to minimize cost
#***********************************************************************************************************************************
#Bellman avec gamma=0.9
#*************************************************************************************************************************************
import numpy as np

# le facteur de discount
gamma = 0.9
epsilon= 0.01 #seuil de convergence


#Transition probability function
def transition_probability(state_index, action, next_state_index):
  #extraire le nombre de clients et le nombre de VMs actives de l'index de l'état actuel
  m, n = divmod(state_index, K+1)
  next_m, next_n = divmod(next_state_index, K+1)

  probability=0
  for i in range(b+1):
    new_m = min(B, max(0, m + i - action * d))
    if new_m == next_m and action == next_n :
      probability += PA[i]

  return probability

Q = np.zeros((state_space_size, action_space_size))
delta = float('inf')
iteration=0
list_min = []
while delta > epsilon:
  Q_prev = Q.copy()
  for state_index in range(state_space_size):
    for action in range(action_space_size):
      Q_next=np.zeros(action_space_size)
      for next_state_index in range (state_space_size) :
        prob= transition_probability(state_index, action, next_state_index)
        _,reward = take_action(state_index, action)
        Q_next[action] += reward + gamma*prob*np.max(Q[next_state_index])
      Q[state_index, action] = Q_next[action]
  delta = np.max(np.abs(Q - Q_prev))
  iteration += 1
  print(f'Iteration {iteration}, Delta :{delta}')
  list_min.append(np.min(Q))

policy = np.argmax(Q, axis=1)

print("Q_values:")
print(Q)
print("Politique Optimale:")
print(policy)


# Visualisation de la convergence des Q-values
plt.figure(figsize=(10, 6))
plt.plot(list_min)
plt.title('Convergence de Q-Value')
plt.xlabel('Iterations')
plt.ylabel('Valeur minimale de Q-Value')
plt.grid(True)
plt.show()

#***********************************************************************************************************************************
#Bellman avec gamma=0.5
#*************************************************************************************************************************************

import numpy as np

# le facteur de discount
gamma = 0.5
epsilon= 0.01 #seuil de convergence


#Transition probability function
def transition_probability(state_index, action, next_state_index):
  #extraire le nombre de clients et le nombre de VMs actives de l'index de l'état actuel
  m, n = divmod(state_index, K+1)
  next_m, next_n = divmod(next_state_index, K+1)

  probability=0
  for i in range(b+1):
    new_m = min(B, max(0, m + i - action * d))
    if new_m == next_m and action == next_n :
      probability += PA[i]

  return probability

Q = np.zeros((state_space_size, action_space_size))
delta = float('inf')
iteration=0
list_min = []
while delta > epsilon:
  Q_prev = Q.copy()
  for state_index in range(state_space_size):
    for action in range(action_space_size):
      Q_next=np.zeros(action_space_size)
      for next_state_index in range (state_space_size) :
        prob= transition_probability(state_index, action, next_state_index)
        _,reward = take_action(state_index, action)
        Q_next[action] += reward + gamma*prob*np.max(Q[next_state_index])
      Q[state_index, action] = Q_next[action]
  delta = np.max(np.abs(Q - Q_prev))
  iteration += 1
  print(f'Iteration {iteration}, Delta :{delta}')
  list_min.append(np.min(Q))

policy = np.argmax(Q, axis=1)

print("Q_values:")
print(Q)
print("Politique Optimale:")
print(policy)


# Visualisation de la convergence des Q-values
plt.figure(figsize=(10, 6))
plt.plot(list_min)
plt.title('Convergence de Q-Value')
plt.xlabel('Iterations')
plt.ylabel('Valeur minimale de Q-Value')
plt.grid(True)
plt.show()

# L'algorithme Sarsa sans changement de learning rate (fixe à 0.1)
import numpy as np

# Paramètres d'apprentissage
alpha = 0.1# Taux d'apprentissage
gamma = 0.99  # Facteur de discount
epsilon = 0.1
episodes =10000

# Initialisation de la table Q
Q = np.zeros((state_space_size, action_space_size))

def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(action_space_size)
    else:
        return np.argmax(Q[state])

def termination_condition (state_index):
    m, n = divmod(state_index, K + 1)  # Convertir state_index en nombre de clients (m) et VMs actives (n)
    return m == 0 or m == B

# Définir les configurations des probabilités d'arrivée
configurations_lambda = [2, 4]  # Exemple : lambda_rate = 2 pour une charge plus légère, lambda_rate = 4 pour une charge plus lourde


for lambda_rate in configurations_lambda:
    PA = [poisson.pmf(k, lambda_rate) for k in range(b + 1)]
    PA /= np.sum(PA)

    Q = np.zeros((state_space_size, action_space_size))

    min_q_values_per_episode = []

    # Boucle d'apprentissage SARSA
    for episode in range(episodes):
        # Initialiser l'état S
        state_index = np.random.randint(state_space_size)
        # Sélectionner l'action A avec la politique epsilon-greedy
        action = epsilon_greedy(Q, state_index, epsilon)

        while True:
            # Exécuter l'action A, observer R, S'
            next_state_index, reward = take_action(state_index, action)

            # Sélectionner l'action A' depuis S' en utilisant la politique epsilon-greedy
            next_action = epsilon_greedy(Q, next_state_index, epsilon)
            # Mettre à jour Q(S,A)
            Q[state_index, action] += alpha * (reward + gamma * Q[next_state_index, next_action] - Q[state_index, action])
            state_index, action = next_state_index, next_action
            # Terminer si S est un état terminal
            if termination_condition(state_index):
                break
        min_q_values_per_episode.append(np.min(Q))


  # Visualisation de la convergence
    plt.figure(figsize=(10, 6))
    plt.plot(min_q_values_per_episode)
    plt.title('Convergence de Q-Value avec ajustement d\'Alpha pour Lambda rate ='+str(lambda_rate))
    plt.xlabel('Épisodes')
    plt.ylabel('Valeur minimale de Q-Value')
    plt.grid(True)
    plt.show()

#***********************************************************************************************************************************
# L'algorithme Sarsa sans changement de learning rate (fixe à 0.9)
#*************************************************************************************************************************************

import numpy as np

# Paramètres d'apprentissage
alpha = 0.9# Taux d'apprentissage
gamma = 0.99  # Facteur de discount
epsilon = 0.1
episodes =10000

# Initialisation de la table Q
Q = np.zeros((state_space_size, action_space_size))

def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(action_space_size)
    else:
        return np.argmax(Q[state])

def termination_condition (state_index):
    m, n = divmod(state_index, K + 1)  # Convertir state_index en nombre de clients (m) et VMs actives (n)
    return m == 0 or m == B

# Définir les configurations des probabilités d'arrivée
configurations_lambda = [2, 4]  # Exemple : lambda_rate = 2 pour une charge plus légère, lambda_rate = 4 pour une charge plus lourde


for lambda_rate in configurations_lambda:
    PA = [poisson.pmf(k, lambda_rate) for k in range(b + 1)]
    PA /= np.sum(PA)

    Q = np.zeros((state_space_size, action_space_size))

    min_q_values_per_episode = []

    # Boucle d'apprentissage SARSA
    for episode in range(episodes):
        # Initialiser l'état S
        state_index = np.random.randint(state_space_size)
        # Sélectionner l'action A avec la politique epsilon-greedy
        action = epsilon_greedy(Q, state_index, epsilon)

        while True:
            # Exécuter l'action A, observer R, S'
            next_state_index, reward = take_action(state_index, action)

            # Sélectionner l'action A' depuis S' en utilisant la politique epsilon-greedy
            next_action = epsilon_greedy(Q, next_state_index, epsilon)
            # Mettre à jour Q(S,A)
            Q[state_index, action] += alpha * (reward + gamma * Q[next_state_index, next_action] - Q[state_index, action])
            state_index, action = next_state_index, next_action
            # Terminer si S est un état terminal
            if termination_condition(state_index):
                break
        min_q_values_per_episode.append(np.min(Q))


  # Visualisation de la convergence
    plt.figure(figsize=(10, 6))
    plt.plot(min_q_values_per_episode)
    plt.title('Convergence de Q-Value avec ajustement d\'Alpha pour Lambda rate ='+str(lambda_rate))
    plt.xlabel('Épisodes')
    plt.ylabel('Valeur minimale de Q-Value')
    plt.grid(True)
    plt.show()


#***********************************************************************************************************************************
# L'algorithme Sarsa avec changement de learning rate
#*************************************************************************************************************************************

import numpy as np

# Paramètres d'apprentissage
alpha_initiale = 0.1# Taux d'apprentissage
alpha_min=0.01
alpha_decay =0.995
gamma = 0.9  # Facteur de discount
epsilon = 0.1
episodes =20000

lambda_rate = 2
PA = [poisson.pmf(k, lambda_rate) for k in range(b + 1)]
PA /= np.sum(PA)
# Initialisation de la table Q
Q = np.zeros((state_space_size, action_space_size))

def termination_condition (state_index):
    m, n = divmod(state_index, K + 1)  # Convertir state_index en nombre de clients (m) et VMs actives (n)
    return m == 0 or m == B


def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(action_space_size)
    else:
        return np.argmax(Q[state])

min_q_values_per_episode = []
# Boucle d'apprentissage SARSA
for episode in range(episodes):
    # Initialiser l'état S
    state_index = np.random.randint(state_space_size)
    # Sélectionner l'action A avec la politique epsilon-greedy
    action = epsilon_greedy(Q, state_index, epsilon)

    while not termination_condition(state_index):
        # Exécuter l'action A, observer R, S'
        next_state_index, reward = take_action(state_index, action)

        # Sélectionner l'action A' depuis S' en utilisant la politique epsilon-greedy
        next_action = epsilon_greedy(Q, next_state_index, epsilon)

        alpha = max(alpha_min, alpha_initiale * (alpha_decay ** episode))  # Décroissance d'alpha

        # Mettre à jour Q(S,A)
        Q[state_index, action] += alpha * (reward + gamma * Q[next_state_index, next_action] - Q[state_index, action])

        state_index, action = next_state_index, next_action
    min_q_values_per_episode.append(np.min(Q))


plt.figure(figsize=(10, 6))
plt.plot(min_q_values_per_episode, label='Valeur Q minimale')
plt.xlabel('Épisode')
plt.ylabel('Valeur Q minimale')
plt.title('Évolution de la valeur Q minimale au fil des épisodes')
plt.legend()
plt.show()

#***********************************************************************************************************************************
#algorithme de Q_learning
#*************************************************************************************************************************************

import numpy as np

# Paramètres d'apprentissage
gamma = 0.9  # Facteur de discount
alpha = 0.1  # Taux d'apprentissage initial
alpha_min=0.1
alpha_decay =0.995
epsilon = 0.1  # Taux d'exploration initial
episodes = 1000  # Nombre total d'épisodes

def termination_condition (state_index):
    m, n = divmod(state_index, K + 1)  # Convertir state_index en nombre de clients (m) et VMs actives (n)
    return m == 0 or m == B

# Fonction de sélection d'action epsilon-greedy
def epsilon_greedy(Q, state, epsilon):
    if np.random.random() < epsilon:
        return np.random.randint(action_space_size)
    else:
        return np.argmax(Q[state])

# Algorithme Q-learning
for lambda_rate in configurations_lambda:
    PA = [poisson.pmf(k, lambda_rate) for k in range(b + 1)]
    PA /= np.sum(PA)

    Q = np.zeros((state_space_size, action_space_size))
    min_q_values_per_episode = []
    for episode in range(episodes):
        state = np.random.randint(state_space_size)  # Initialiser l'état S
        done = False
        while not done:
            action = epsilon_greedy(Q, state, epsilon)  # Choisir une action A
            next_state, reward = take_action(state, action)  # Observer l'état S' et la récompense R
            # Mettre à jour Q(S,A)
            alpha = max(alpha_min, alpha* (alpha_decay ** episode))  # Décroissance d'alpha
            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
            state = next_state  # S <- S'
            done = termination_condition(state)  # Vérifier la condition de terminaison

        min_q_values_per_episode.append(np.min(Q))


    plt.figure(figsize=(10, 6))
    plt.plot(min_q_values_per_episode, label='Valeur Q minimale')
    plt.xlabel('Épisode')
    plt.ylabel('Valeur Q minimale')
    plt.title('Convergence de Q-Value avec ajustement d\'Alpha pour Lambda rate ='+str(lambda_rate))
    plt.legend()
    plt.show()

#***********************************************************************************************************************************
#algorithme de Q_learning
#*************************************************************************************************************************************
import numpy as np

# Paramètres d'apprentissage
gamma = 0.99 # Facteur de discount
alpha = 0.1 # Taux d'apprentissage initial
alpha_min=0.01
alpha_decay =0.995
epsilon = 0.1  # Taux d'exploration initial
episodes = 1000  # Nombre total d'épisodes
lambda_rate = 2  # Average rate of packet arrivals per time slot

PA = [poisson.pmf(k, lambda_rate) for k in range(b + 1)]
PA /= np.sum(PA)
# Initialisation de la table Q
Q = np.zeros((state_space_size, action_space_size))

def termination_condition (state_index):
    m, n = divmod(state_index, K + 1)  # Convertir state_index en nombre de clients (m) et VMs actives (n)
    return m == 0 or m == B

# Fonction de sélection d'action epsilon-greedy
def epsilon_greedy(Q, state, epsilon):
    if np.random.random() < epsilon:
        return np.random.randint(action_space_size)
    else:
        return np.argmax(Q[state])

# Algorithme Q-learning
min_q_values_per_episode = []
for episode in range(episodes):
    state = np.random.randint(state_space_size)  # Initialiser l'état S
    done = False
    while not done:
        action = epsilon_greedy(Q, state, epsilon)  # Choisir une action A
        next_state, reward = take_action(state, action)  # Observer l'état S' et la récompense R
        # Mettre à jour Q(S,A)
        alpha = max(alpha_min, alpha * (alpha_decay ** episode))  # Décroissance d'alpha
        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        state = next_state  # S <- S'
        done = termination_condition(state)  # Vérifier la condition de terminaison

    min_q_values_per_episode.append(np.min(Q))


plt.figure(figsize=(10, 6))
plt.plot(min_q_values_per_episode, label='Valeur Q minimale')
plt.xlabel('Épisode')
plt.ylabel('Valeur Q minimale')
plt.title('Évolution de la valeur Q minimale au fil des épisodes')
plt.legend()
plt.show()